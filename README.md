# EXP-1-PROMPT-ENGINEERING-

## Aim: 
Comprehensive Report on the Fundamentals of Generative AI and Large Language Models (LLMs)
Experiment: Develop a comprehensive report for the following exercises:

Explain the foundational concepts of Generative AI.
Focusing on Generative AI architectures. (like transformers).
Generative AI applications.
Generative AI impact of scaling in LLMs.

## Algorithm:
1. Study fundamentals of Generative AI.
2. Differentiate discriminative and generative models.
3. Understand generative architectures (RNN → LSTM → Transformer).
4. Explore Transformer components (Attention, Encoder-Decoder).
5. Identify applications of Generative AI in various domains.
6. Analyze scaling laws and impact on LLMs.
7. Summarize findings with real-world examples.

## Output
1.Generative AI can generate realistic new data such as human-like text, lifelike images, synthetic voices, and even protein structures.

2.Transformers solved the limitations of RNNs and LSTMs by introducing attention, enabling models to handle long sequences efficiently.

3.Large Language Models (LLMs) like GPT, BERT, and PaLM show advanced capabilities in text comprehension, reasoning, coding, and conversation.

4.Applications observed:
• Text generation → ChatGPT, summarizers, translators.
• Image generation → DALL·E, MidJourney, Stable Diffusion.
• Code generation → GitHub Copilot, Codex.
• Healthcare → AlphaFold for protein folding, AI in drug design.
• Creative industries → script writing, music generation, design.

5.Scaling Impact:
• Larger models demonstrate “emergent abilities” (solving tasks they were not directly trained for).
• Performance improves predictably with more data, compute, and parameters (Scaling Laws).
• Example: GPT-2 (1.5B parameters) → GPT-3 (175B parameters) → GPT-4 (multi-modal, reasoning).
• Challenges include high energy use, environmental cost, training data bias, and ethical misuse.


## Result
The experiment successfully explains the fundamentals of Generative AI.


